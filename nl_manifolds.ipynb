{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-driven nonlinear manifolds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A general representation learning problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate the construction of data-driven nonlinear manifolds for model reduction purposes. We learn the manifold through the identification of low-order, polynomial structure in the data through a general representation representation learning problem.<sup>[1](https://doi.org/10.48550/arXiv.2306.13748)</sup> In particular, we seek to build approximations of the form\n",
    "\n",
    "$$\n",
    "\\mathbf{s}_j \\approx \\mathbf{s}_\\text{ref} + \\mathbf{V} \\hat{\\mathbf{s}}_j + \\overline{\\mathbf{V}} \\boldsymbol{\\Xi} \\mathbf{g}(\\hat{\\mathbf{s}}_j), \\quad \\text{for } j=1,\\dots,k,\n",
    "$$\n",
    "\n",
    "where $\\mathbf{s}_j \\in \\mathbb{R}^{n}$ is the $j$th data sample, $\\mathbf{s}_\\text{ref} \\in \\mathbb{R}^{n}$ is a given reference state, $\\mathbf{V} \\in \\mathbb{R}^{n \\times r}$ and $\\overline{\\mathbf{V}} \\in \\mathbb{R}^{n \\times q}$ are basis matrices, $\\hat{\\mathbf{s}}_j \\in \\mathbb{R}^{n \\times r}$ is the reduced-state representation of the $j$th snapshot, and $\\boldsymbol{\\Xi} \\in \\mathbb{R}^{q \\times (p-1)r}$ is the coefficient matrix. The total number of data samples is given by $k$. The vector $\\mathbf{g}(\\hat{\\mathbf{s}}(t)) \\in \\mathbb{R}^{(p-1)r}$ has the form\n",
    "\n",
    "$$\n",
    "\\mathbf{g}(\\hat{\\mathbf{s}}(t)) = \\begin{pmatrix} \\hat{\\mathbf{s}}^2(t) \\\\\n",
    "\\hat{\\mathbf{s}}^3(t) \\\\\n",
    "\\vdots                \\\\\n",
    "\\hat{\\mathbf{s}}^p(t) \n",
    "\\end{pmatrix},\n",
    "\\label{eq:polynomial}\n",
    "$$\n",
    "\n",
    "where each $\\hat{\\mathbf{s}}^j(t) \\in \\mathbb{R}^r$ consists of the $j$th power of the components of $\\hat{\\mathbf{s}}(t)$, that is, $\\hat{\\mathbf{s}}^j(t) = [\\hat{s}_1(t)^j,\\hat{s}_2(t)^j,\\dotsc,\\hat{s}_r(t)^j]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem setup & numerical implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the needed things from NumPy, Matplotlib and SciPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the data by sampling the vector $\\mathbf{s}(x,y)=\\left(x,y,\\sin(x)\\cos(y)\\right)^\\top \\in \\mathbb{R}^3$ for $x,y \\in [0,4]$. For centering the data we define the reference state $\\mathbf{s}_\\text{ref}$ as the column-averaged mean of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mesh grid\n",
    "nx, ny = (41, 41)\n",
    "num_snapshots = nx * ny\n",
    "x = np.linspace(0, 4, nx)\n",
    "y = np.linspace(0, 4, ny)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "# compute data matrix\n",
    "fxy = np.sin(xx) * np.cos(yy)\n",
    "S = np.reshape([xx, yy, fxy], (3, num_snapshots))\n",
    "sref = np.mean(S, axis=1)\n",
    "Sref = np.array([sref,] * num_snapshots).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 3           # degree of polynomial embeddings\n",
    "r = 2           # number of basis vectors in Vr\n",
    "q = 1           # number of basis vectors in Vbar\n",
    "tol = 1e-3      # tolerence for alternating minimization\n",
    "gamma = 0       # regularization parameter\n",
    "max_iter = 100  # maximum number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a number of useful function handles for quickly evaluating the function $\\mathbf{g}(\\hat{\\mathbf{s}})$ and computing the relative state error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_form(x):\n",
    "    \"\"\"Polynomial representation of the reduced state snapshot data.\"\"\"\n",
    "    return [x**degree for degree in range(2, p+1)]\n",
    "\n",
    "\n",
    "def relative_error(S_exact, S_reconstructed):\n",
    "    \"\"\"Calculate the relative squared Frobenius-norm error.\"\"\"\n",
    "    return np.linalg.norm(S_exact - S_reconstructed, 'fro') / np.linalg.norm(S_exact - Sref, 'fro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a function for the visualizuation of the reconstructed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstruction(Gamma):\n",
    "    \"\"\"Plot the original data (fxy) and the reconstruction (fxy_Gamma).\"\"\"\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    fxy_Gamma = np.reshape(Gamma[2,::], (nx, ny))\n",
    "    ax.plot_surface(xx, yy, fxy, color='gray', edgecolor='black', linewidth=0.1, alpha=0.2)\n",
    "    surf = ax.plot_surface(xx, yy, fxy_Gamma, cmap='seismic', vmin=-1, vmax=1, alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_zlabel('$f(x,y)$')\n",
    "    ax.set_xlim(0, 4)\n",
    "    ax.set_ylim(0, 4)\n",
    "    ax.set_zlim(-1, 1)\n",
    "    ax.view_init(30, 130)\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=20, pad=0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparison of representation learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Traditional POD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a simple POD method. We first compute the left singular vectors of the shifted data matrix. A two-dimensional representation of the data is then obtained by means of a projection onto the two-dimensional POD subspace. The corresponding approximation of the $j$th snapshot given by\n",
    "\n",
    "$$\n",
    "\\mathbf{s}_j \\approx \\mathbf{s}_\\text{ref} + \\mathbf{V}\\hat{\\mathbf{s}}_j, \\quad \\text{for } j=1,\\dots,k.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, ss, _ = np.linalg.svd(S - Sref, full_matrices=False)\n",
    "Vr = U[:, :r]                   # select r leading singular vectors\n",
    "Shat = Vr.T @ (S-Sref)          # represent data in POD coordinates\n",
    "\n",
    "Gamma_POD = Sref + Vr @ Shat    # compute the projection of the original dataset\n",
    "print(f\"\\nReconstruction error: {relative_error(S, Gamma_POD):.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the reconstructions obtained using POD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstruction(Gamma_POD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. POD-based nonlinear manifold learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the POD-based manifold learning approach we choose the columns of $\\mathbf{V}$ to be, as is the case in the traditional POD approach, the dominant two left singular vectors of the shifted data. The third singular vector now be contained in $\\overline{\\mathbf{V}}$. The coefficient matrix $\\boldsymbol{\\Xi} \\in \\mathbb{R}^{q \\times (p-1)r}$  is computed via the normal equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vbar = U[:, r:r+q]\n",
    "Proj_error = S - Sref - (Vr @ Shat)\n",
    "Poly = np.concatenate(polynomial_form(Shat), axis=0)\n",
    "Xi = Vbar.T @ Proj_error @ Poly.T @ np.linalg.inv(Poly @ Poly.T + gamma * np.identity((p - 1)*r))\n",
    "\n",
    "Gamma_MPOD = Sref + (Vr @ Shat) + (Vbar @ Xi @ Poly)\n",
    "print(f\"\\nReconstruction error: {relative_error(S, Gamma_MPOD):.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the reconstructed manifold using the nonlinear manifold based POD formulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstruction(Gamma_MPOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Alternating mimization based nonlinear manifold learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is successively minimized for the three blocks of variables in turn $-$first $\\boldsymbol{\\Omega} = (\\mathbf{V}, \\overline{\\mathbf{V}} )$, then $\\boldsymbol{\\Xi}$, then $\\hat{\\mathbf{S}}$$-$ with the pattern repeating until a convergence criterion is satisfied. The minimization  with respect to $\\boldsymbol{\\Omega}$ is a standard problem known as the orthogonal Procrustes problem. The minimization with respect to $\\boldsymbol{\\Xi}$ is a linear least squares problem, as in above approach. The minimization with respect to $\\hat{\\mathbf{S}}$ decomposes into $k$ separate problems, each of which has a single reduced  state $\\hat{\\mathbf{s}}_j$ as its variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation_learning_obj(x):\n",
    "    \"\"\"Objective function for the nonlinear regression problem in the alternating minimization approach.\"\"\"\n",
    "    return S[:, snapshot] - sref - (Vr @ x) - (Vbar @ Xi @ np.concatenate(polynomial_form(x), axis=0))\n",
    "\n",
    "\n",
    "nrg_old = 0\n",
    "print(\"***Starting alternating minimizations:\")\n",
    "\n",
    "# start iterations\n",
    "for niter in range(max_iter):\n",
    "\n",
    "    # step 1 - orthogonal Procrustes (update basis vectors)\n",
    "    Um, _ , Vm = np.linalg.svd((S - Sref) @ np.concatenate([Shat, Xi @ Poly]).T, full_matrices=False)\n",
    "    Omega = Um @ Vm\n",
    "    Vr, Vbar = Omega[:, :r], Omega[:, r:r+q]\n",
    "\n",
    "    # step 2 - linear regression (update coefficient matrix)\n",
    "    Proj_error = S - Sref - (Vr @ Shat)\n",
    "    rhs = np.linalg.inv(Poly @ Poly.T + (gamma * np.identity((p - 1)*r)))\n",
    "    Xi = Vbar.T @ Proj_error @ Poly.T @ rhs\n",
    "\n",
    "    # step 3 - nonlinear regression (update reduced state representation)\n",
    "    for snapshot in range(num_snapshots):\n",
    "        Shat[:, snapshot] = opt.least_squares(representation_learning_obj, Shat[:, snapshot], ftol=1e-9).x\n",
    "    Poly = np.concatenate(polynomial_form(Shat), axis=0)\n",
    "\n",
    "    # evaluate convergence criterion\n",
    "    energy = np.linalg.norm(Vr @ Shat + (Vbar @ Xi @ Poly), 'fro')**2 / np.linalg.norm(S - Sref, 'fro')**2\n",
    "    diff = abs(energy - nrg_old)\n",
    "    print(f\"\\titeration: {niter+1:d}\\tsnapshot energy: {energy:e}\\t diff: {diff:e}\")\n",
    "    if diff < tol:\n",
    "        print(\"***Convergence criterion active!\")\n",
    "        break\n",
    "    nrg_old = energy # update old energy metric\n",
    "\n",
    "\n",
    "Gamma_MAM = Sref + (Vr @ Shat) + (Vbar @ Xi @ Poly)\n",
    "print(f\"\\nReconstruction error: {relative_error(S, Gamma_MAM):.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the reconstructed manifold using the alternating mimization formulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstruction(Gamma_MAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Useful references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1. Geelen, R., Balzano, L. and Willcox, K., 2023. Learning latent representations in high-dimensional state spaces using polynomial manifold constructions. arXiv preprint arXiv:2306.13748. [doi.org/10.48550/arXiv.2306.13748](https://doi.org/10.48550/arXiv.2306.13748)\n",
    "  2. Geelen, R., Balzano, L., Wright, S. and Willcox, K., 2023. Learning physics-based reduced-order models from data using nonlinear manifolds.\n",
    "  3. Geelen, R., Wright, S. and Willcox, K., 2023. Operator inference for non-intrusive model reduction with quadratic manifolds. Computer Methods in Applied Mechanics and Engineering, 403, p.115717. [doi.org/10.1016/j.cma.2022.115717](https://doi.org/10.1016/j.cma.2022.115717)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
